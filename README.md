<h1 align="center"><strong>RAMEN: Real-time Asynchronous Multi-agent Neural Implicit Mapping</strong></h1>

<p align="center">
	<a href="https://scholar.google.com/citations?user=4uQNsj8AAAAJ&hl=zh-CN">Hongrui Zhao</a>, 
	<a href="https://www.borisivanovic.com/">Boris Ivanovic</a>,
	<a href="https://negarmehr.com/">Negar Mehr</a>,
</p>

<div align="center">
	<a href='https://arxiv.org/abs/2502.19592'><img src='https://img.shields.io/badge/arXiv-2308.16246-b31b1b'></a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
 	<a href='https://iconlab.negarmehr.com/RAMEN/'><img src='https://img.shields.io/badge/Project-Page-Green'></a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
 	<!-- <a href='https://www.youtube.com/watch?v=psPvanfh7SA&feature=youtu.be'><img src='https://img.shields.io/badge/Youtube-Video-blue'></a> -->
</div>


## Installation
### (1) Install basic python packages
First clone the repositroy and create the conda environment.
```shell
git clone https://github.com/labicon/RAMEN.git
cd RAMEN
# create conda env
conda create -n RAMEN python=3.10
conda activate RAMEN
```
Now install pytorch. RAMEN has been tested on CUDA 11.8 with RTX4090 & RTX2060M, and CUDA 12.8 (Nightly) with RTX5090.   
Below is for CUDA 12.8 (Nightly). You can install different CUDA from <a href="https://pytorch.org/get-started/locally/">here</a>.
```shell
pip3 install --pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/cu128  
```
Install other python packages for visualization, mesh generation, and configuration files.
```shell
pip install PyMCubes open3d==0.18.0 trimesh opencv-python matplotlib pyyaml
```

### (2) Build tinycudann from source
Make sure your CMake is 3.21 or higher, and your gcc is 11 or higher.  
Now we will build tinycudann from source. It is going to take a while. As long as you are seeing ```Building wheel for tinycudann (setup.py) ... ```, it's all good. 
```shell
pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch
```


### (3) Build pytorch3d from source
First install pytorch3d's dependecies
```shell
pip install -U iopath
```
Now we will build pytorch3d from source. It is going to take a while. As long as you are seeing ```Building wheel for pytorch3d  (setup.py) ... ```, it's all good. 
```shell
 pip install "git+https://github.com/facebookresearch/pytorch3d.git"
```



## Dataset
### (1) Replica

Download the sequences of the Replica Dataset generated by the authors of iMAP into `./data/Replica` folder. 

```bash
bash scripts/download_replica.sh # Released by authors of NICE-SLAM
```

### (2) ScanNet

Please follow the procedure on [ScanNet](http://www.scan-net.org/) website, and extract color & depth frames from the `.sens` file using the [code](https://github.com/ScanNet/ScanNet/blob/master/SensReader/python/reader.py).




## Run 
### (1) Config files
Config yaml files for different scenes are located at ```/configs``` folder.  
Some config parameters of your interest:
* ```data.exp_name```: results will be saved at path ``` data.output + data.exp_name```.
* ```multi_agents.complete_graph```: if set to ```True```, the agent communication graph will be a complete graph (every agent can talk to every agent).
* ```multi_agents.edges_list```: if ```complete_graph``` is set to ```False```, this will define the comuunication edges between agents.
* ```multi_agents.edges_for_dropout```: A list of [node i, node j, possibility of dropping out] to define the communication dropout rate. Empty list means no communication dropout. 
* ```multi_agents.distributed_algorithm```: To run RAMEN, set it to `AUQ_CADMM`. Other baseline methods are `CADMM` (this is DiNNO in our paper), `MACIM`, `DSGD`, `DSGT`.
* ```multi_agents.fix_decoder```: need to set to ```True``` to run RAMEN.

### (2) Run main.py
Let's run office 1 from replica dataset
```shell
python main.py --config configs/Replica/office1.yaml
```




## Visualization and evaluation
### (1) Visualization
```shell
python visualizer.py --config ./configs/Replica/office1.yaml 
```

### (2) Evaluation
Check out `analysis.ipynb`.

 
## Acknowledgement

Our code is partially based on [Co-SLAM](https://github.com/HengyiWang/Co-SLAM). We thank the authors for making these codes publicly available.

## Citation

```
@inproceedings{Zhao2025RSS
  title={RAMEN: Real-time Asynchronous Multi-agent Neural Implicit Mapping},
  author={Zhao, Hongrui and Ivanovic, Boris and Mehr, Negar},
  booktitle={Robotics: Science and Systems (RSS)},
  year={2025}
}
```
