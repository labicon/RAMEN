{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from itertools import combinations \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import networkx as nx \n",
    "import argparse\n",
    "import config\n",
    "import os \n",
    "\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1, Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We usally run >3 tests to get avergaed peformance over random network initalization and random communication dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = 'scannet'\n",
    "scene = 'scene0000'\n",
    "#last_mesh_id = 773 # scene0106 \n",
    "last_mesh_id = 1858 # scene0000\n",
    "\n",
    "# data_set = 'Replica'\n",
    "# scene = 'office1'\n",
    "# last_mesh_id = 665 # room1\n",
    "\n",
    "#exp_name = 'DiNNO_D50'\n",
    "#exp_name = 'Ours_D50'\n",
    "exp_name = 'DSGD_D50'\n",
    "#exp_name = 'DSGT_D50'\n",
    "\n",
    "num_of_agents = 3\n",
    "number_of_trials = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For scannet, we cull centralized mesh, and use it as the ground truth mesh  \n",
    "simply copy outputs to terminal to run them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last_centralized = 2019\n",
    "last_centralized = 5577 # scene0000\n",
    "#last_centralized = 2323 # scene0106\n",
    "print(f'INPUT_MESH=output/scannet/{scene}_00/Centralized/agent_0/mesh_track{last_centralized}.ply')\n",
    "print(f'python cull_mesh.py --config configs/scannet/{scene}.yaml --input_mesh $INPUT_MESH --remove_occlusion --gt_pose')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cull meshes and get evaluation scores  \n",
    "Copy outputs into ```evaluation.sh```  \n",
    "First remove previous log file  ```rm -rf screen.log```.  \n",
    "Then run ```script screen.log``` to log terminal output. You need to reactivate conda env after running this.   \n",
    "Then run evaluation by ```bash evaluation.sh```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_set == 'scannet':\n",
    "    for agent_id in range(num_of_agents):\n",
    "        for i in range(number_of_trials):\n",
    "            print('\\n')\n",
    "            print(f'INPUT_MESH=output/scannet/{scene}_00/{exp_name}_{i+1}/agent_{agent_id}/mesh_track{last_mesh_id}.ply')\n",
    "            print(f'python cull_mesh.py --config configs/scannet/{scene}.yaml --input_mesh $INPUT_MESH --remove_occlusion --gt_pose')\n",
    "            print('\\n')\n",
    "            print(f'REC_MESH=output/scannet/{scene}_00/{exp_name}_{i+1}/agent_{agent_id}/mesh_track{last_mesh_id}_cull_occlusion.ply')\n",
    "            print(f'GT_MESH=output/scannet/{scene}_00/Centralized/agent_0/mesh_track{last_centralized}_cull_occlusion.ply')\n",
    "            print(f'python eval_recon.py --rec_mesh $REC_MESH --gt_mesh $GT_MESH -3d')\n",
    "            print('\\n')\n",
    "elif data_set =='Replica':\n",
    "    for agent_id in range(num_of_agents):\n",
    "        for i in range(number_of_trials):\n",
    "            print('\\n')\n",
    "            print(f'INPUT_MESH=output/Replica/{scene}/{exp_name}_{i+1}/agent_{agent_id}/mesh_track{last_mesh_id}.ply')\n",
    "            print(f'VIRT_CAM_PATH=eval_data/Replica/{scene}/virtual_cameras')\n",
    "            print(f'python cull_mesh.py --config configs/Replica/{scene}.yaml --input_mesh $INPUT_MESH --remove_occlusion --virtual_cameras --virt_cam_path $VIRT_CAM_PATH --gt_pose')\n",
    "            print('\\n')\n",
    "            print(f'REC_MESH=output/Replica/{scene}/{exp_name}_{i+1}/agent_{agent_id}/mesh_track665_cull_virt_cams.ply')\n",
    "            print(f'GT_MESH=eval_data/Replica/{scene}/gt_mesh_cull_virt_cams.ply')\n",
    "            print(f'python eval_recon.py --rec_mesh $REC_MESH --gt_mesh $GT_MESH --dataset_type Replica -3d') # add '-2d' to get depth L1 loss\n",
    "            print('\\n')\n",
    "else:\n",
    "    print('WARNING: unsupported dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First stop screeg.log by running ```exit```.  \n",
    "The code below fethc evaluation results from ```screen.log```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metrics(filename=\"screen.log\"):\n",
    "  \"\"\"\n",
    "  Extracts accuracy, completion, and completion ratio values from a log file.\n",
    "\n",
    "  Args:\n",
    "    filename: The path to the log file.\n",
    "\n",
    "  Returns:\n",
    "    A tuple containing three NumPy arrays: accuracy_values, completion_values, \n",
    "    and completion_ratio_values.\n",
    "  \"\"\"\n",
    "  accuracy_values = []\n",
    "  completion_values = []\n",
    "  completion_ratio_values = []\n",
    "\n",
    "  try:\n",
    "    with open(filename, \"r\") as file:\n",
    "      for line in file:\n",
    "        # Use regular expressions to find the values\n",
    "        accuracy_match = re.search(r\"accuracy: (\\d+\\.?\\d*)\", line)\n",
    "        completion_match = re.search(r\"completion: (\\d+\\.?\\d*)\", line)\n",
    "        completion_ratio_match = re.search(r\"completion ratio: (\\d+\\.?\\d*)\", line)\n",
    "\n",
    "        if accuracy_match:\n",
    "          accuracy_values.append(float(accuracy_match.group(1)))\n",
    "        if completion_match:\n",
    "          completion_values.append(float(completion_match.group(1)))\n",
    "        if completion_ratio_match:\n",
    "          completion_ratio_values.append(float(completion_ratio_match.group(1)))\n",
    "\n",
    "  except FileNotFoundError:\n",
    "    print(f\"Error: File '{filename}' not found.\")\n",
    "    return None, None, None\n",
    "\n",
    "  return np.array(accuracy_values), np.array(completion_values), np.array(completion_ratio_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, completion, completion_ratio = extract_metrics()\n",
    "\n",
    "print('Accuracy: {:.2f} ±{:.2f} '.format(np.mean(accuracy), np.std(accuracy)))\n",
    "print('completion_ratio: {:.2f} ±{:.2f} '.format(np.mean(completion), np.std(completion)))\n",
    "print('completion_ratio: {:.2f} ±{:.2f} '.format(np.mean(completion_ratio), np.std(completion_ratio)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAMEN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
